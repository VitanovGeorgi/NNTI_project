{"cells":[{"cell_type":"markdown","id":"454e0124","metadata":{"id":"454e0124"},"source":["# Speaker-Independent Spoken Digit Recognition (xSDR)\n","\n","\n","One of the successful stories of deep neural networks is the proliferation of commercial of automatic speech recognition (ASR) systems. This project aims to explore one application of ML-powered ASR to the problem of spoken digit recognition (SDR). Since digits are widely used as unique identifiers for bank information, social security numbers, post codes, etc, SDR systems can be an efficient alternative to fully-fledged ASR systems since the domain is more predictable than other applications of ASR. \n","\n","In this project, we focus on developing a SDR system in a speaker-independent setting. That is, the speakers in the evaluation set are disjoint from the training set speakers. We do so because we expect real-world ASR systems to generalize to different speakers than those we have data for. Moreover, for many languages that are under-resourced, we have have (limited) annotated speech data from a single speaker, but we would still want the system to be deployed to work on any speaker of that language. We tackle the problem of spoken digit recognition as a sequence classification task. Concretely, the inputs are short audio clips of a specific digit (in the range 0-9), then the goal is to build deep neural network models to classify a short audio clip and predict the digit that was spoken."]},{"cell_type":"code","execution_count":1,"id":"020b704f","metadata":{"executionInfo":{"elapsed":8000,"status":"ok","timestamp":1679506484683,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"020b704f"},"outputs":[],"source":["#%matplotlib inline\n","import numpy as np\n","import scipy, matplotlib.pyplot as plt, IPython.display as ipd\n","import librosa, librosa.display\n","import skimage.measure\n","import pandas as pd\n","\n","from sklearn  import preprocessing\n","\n","from collections import defaultdict, Counter\n","\n","# add this to ignore warnings from Librosa\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import random\n","\n","import torchaudio\n","import math\n","import os\n","import pathlib\n","import torch"]},{"cell_type":"code","execution_count":2,"id":"2b2719f5","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1679506484684,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"2b2719f5"},"outputs":[],"source":["# for linear models \n","from sklearn.linear_model import SGDClassifier, Ridge, LogisticRegression, Lasso\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n"]},{"cell_type":"code","execution_count":3,"id":"993583d5","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679506484684,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"993583d5"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay"]},{"cell_type":"code","execution_count":4,"id":"f70e4098","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1679506484685,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"f70e4098"},"outputs":[],"source":["from sklearn.neural_network import MLPClassifier"]},{"cell_type":"code","execution_count":null,"id":"DYB7aSFpXg_9","metadata":{"executionInfo":{"elapsed":6670,"status":"ok","timestamp":1679506491350,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"DYB7aSFpXg_9"},"outputs":[],"source":["from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, AveragePooling1D, Flatten\n","import tensorflow as tf\n","# import tensorflow_addons as tfa\n","import tensorflow.keras.backend as K\n","import keras \n","from keras.utils import to_categorical\n","from keras import layers"]},{"cell_type":"code","execution_count":null,"id":"vMIu6zLEdI_1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18959,"status":"ok","timestamp":1679506510303,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"vMIu6zLEdI_1","outputId":"91cf544b-ddae-45ff-bb66-f6bab9c1abf7"},"outputs":[],"source":["!pip install transformers\n","from transformers import ASTForAudioClassification"]},{"cell_type":"code","execution_count":null,"id":"Nue5yoMZky2l","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7069,"status":"ok","timestamp":1679506517363,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"Nue5yoMZky2l","outputId":"455481e6-9bc0-420b-a557-dbc946ce09fb"},"outputs":[],"source":["!pip install deepsig\n","from deepsig import multi_aso "]},{"cell_type":"code","execution_count":null,"id":"8lG7M8Fp64Hx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7592,"status":"ok","timestamp":1679506524949,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"8lG7M8Fp64Hx","outputId":"ffce6463-7185-48d3-899e-1e4ffbab238f"},"outputs":[],"source":["!pip install tensorflow-addons"]},{"cell_type":"markdown","id":"6a0a0884","metadata":{"id":"6a0a0884"},"source":["## Exploring the Dataset \n","\n","The speech samples are already divied into training, development, and test spilts. The splits are made in such way that evaluation speakers are not present in training split. You should use the splits as they are. \n","\n","**CAUTION:** \n","\n","In this project, you are not allowed to use any external data for this problem (at least for the main three tasks). Exploring the effect of additional datasets in this project can only included as a further step after completing the main requirements with the given data. "]},{"cell_type":"code","execution_count":9,"id":"EgwfX5qGSEf9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28070,"status":"ok","timestamp":1679506553011,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"EgwfX5qGSEf9","outputId":"b12cab65-40cc-4ee3-ee70-b2ed4b89b5ed"},"outputs":[],"source":["# read tsv file into a dataframe \n","# from google.colab import drive \n","# drive.mount('/content/drive')\n","# change path to your own drive\n","# path = \"/content/drive/MyDrive/colab_data/\"\n","path = ''\n","sdr_df = pd.read_csv(path + 'SDR_metadata.tsv', sep='\\t', header=0, index_col='Unnamed: 0')"]},{"cell_type":"code","execution_count":null,"id":"jwZOjJCkSRPA","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1679506553012,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"jwZOjJCkSRPA","outputId":"3f614dbf-7468-4e14-fbf1-5987743afb17"},"outputs":[],"source":["sdr_df.head()"]},{"cell_type":"code","execution_count":null,"id":"4c34786a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1679506553013,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"4c34786a","outputId":"76b1f536-826c-403e-a086-c01628659779"},"outputs":[],"source":["set(sdr_df.speaker.values)"]},{"cell_type":"code","execution_count":null,"id":"155ea375","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1679506553013,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"155ea375","outputId":"50654d14-3476-44f7-9243-b704943990bb"},"outputs":[],"source":["# explore one sample: 7_theo_0\n","sdr_df.loc[sdr_df['identifier'] == '7_theo_0']"]},{"cell_type":"code","execution_count":13,"id":"e03e0920","metadata":{"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1679506554327,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"e03e0920"},"outputs":[],"source":["sample_wav_file = sdr_df.loc[sdr_df['identifier'] == '7_theo_0'].file[700]"]},{"cell_type":"markdown","id":"6ab5f7e7","metadata":{"id":"6ab5f7e7"},"source":["## The Speech Waveform\n","\n","The acoustic realization of speech segment can be (digitally) viewed as a time-variant wavform $\\mathbf{S} \\in \\mathbb{R}^{n}$. Here, $n$ depends on both the duration of the speech segment and the sampling rate of the continous speech singal. Let's check out one sample from the data set. "]},{"cell_type":"code","execution_count":null,"id":"ac93353b","metadata":{"id":"ac93353b"},"outputs":[],"source":["# play and listen to a sample \n","SAMPLING_RATE = 8000 # This value is determined by the wav file, DO NOT CHANGE\n","\n","x, sr = librosa.load(path + sample_wav_file, sr=SAMPLING_RATE) #, \n","ipd.Audio(x, rate=sr)"]},{"cell_type":"code","execution_count":null,"id":"b7438910","metadata":{"id":"b7438910"},"outputs":[],"source":["# plot as a waveform \n","fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n","\n","img = librosa.display.waveshow(y=x, sr=sr, alpha=0.75, x_axis='time', color='blue')\n","\n","ax.set(title='Amplitude waveform')\n","ax.set_ylabel('Amplitude')\n","ax.label_outer()"]},{"cell_type":"code","execution_count":null,"id":"3c4297e5","metadata":{"id":"3c4297e5"},"outputs":[],"source":["# sample duration in milliseconds\n","(1000*len(x))/SAMPLING_RATE"]},{"cell_type":"markdown","id":"3df44120","metadata":{"id":"3df44120"},"source":["In the cell above, you can see the temporal duration of the audio is 428.5 milliseconds. For digits in the range 0-9, the duration of the speech segment should be around 0.5 seconds with reasonable variation depending on speech rate (i.e., how fast the speaker speaks). "]},{"cell_type":"markdown","id":"d2a0c08e","metadata":{"id":"d2a0c08e"},"source":["## The Speech Signal Representation - Mel Spectrograms"]},{"cell_type":"markdown","id":"6370d9c2","metadata":{"id":"6370d9c2"},"source":["Humans can recognize and differentiate different speech sounds based on the frequency characteristics of the sounds. For machine learning applications, human speech is represented using spectro-temporal features in the [Mel-scale](https://en.wikipedia.org/wiki/Mel_scale) extracted from the speech sample. Mel-scale features are inspired by human speech perception and auditory processing whereby the human ear has difference sensitivity (or resolution) in differet frequency bandes. That is, the human ear can better recognize differences in in lower range frequences, while higher range frequences have a lower resolution. The Mel-scale is linear for frequencies in the range (0-1kHz), and logarithmic for frequencies above 1kHz.\n","\n","In the spectro-temporal representation of speech, a speech sample can be seen as a sequence of $T$ spectral vectors as $\\mathbf{X} = (\\mathbf{x}^1, \\mathbf{x}^2, \\dots, \\mathbf{x}^T)$. Each spectral vector $\\mathbf{x}^t \\in \\mathbb{R}^{k}$ at time-step $t$ is extracted from a short speech segment (~25 milliseconds) with the assumption that the signal is time-invariant in this small time window. Here, $k$ is the number of frequency bands in the [spectrogram](https://en.wikipedia.org/wiki/Spectrogram) and this is a parameter of the feature extraction pipeline. The representation is based on the Fourier transform to convert the temporal signal into the frequency domain. \n","\n","In automatic speech recognition (ASR) research and applications, spectral vectors are usually referred to as \"acoustic frames\". Morover, adjacent frames are extracted with some overlap between them, usually ~10 milliseconds. "]},{"cell_type":"code","execution_count":17,"id":"ef41492d","metadata":{"id":"ef41492d"},"outputs":[],"source":["def extract_melspectrogram(signal, sr, num_mels):\n","    \"\"\"\n","    Given a time series speech signal (.wav), sampling rate (sr), \n","    and the number of mel coefficients, return a mel-scaled \n","    representation of the signal as numpy array.\n","    \"\"\"\n","    \n","    mel_features = librosa.feature.melspectrogram(y=signal,\n","        sr=sr,\n","        n_fft=200, # with sampling rate = 8000, this corresponds to 25 ms\n","        hop_length=80, # with sampling rate = 8000, this corresponds to 10 ms\n","        n_mels=num_mels, # number of frequency bins, use either 13 or 39\n","        fmin=50, # min frequency threshold\n","        fmax=4000 # max frequency threshold, set to SAMPLING_RATE/2\n","    )\n","    \n","    # for numerical stability added this line\n","    mel_features = np.where(mel_features == 0, np.finfo(float).eps, mel_features)\n","\n","    # 20 * log10 to convert to log scale\n","    log_mel_features = 20*np.log10(mel_features)\n","\n","    # feature scaling\n","    scaled_log_mel_features = preprocessing.scale(log_mel_features, axis=1)\n","    \n","    return scaled_log_mel_features"]},{"cell_type":"code","execution_count":null,"id":"3600d78e","metadata":{"id":"3600d78e"},"outputs":[],"source":["melspectrogram = extract_melspectrogram(x, sr, num_mels=13)\n","\n","melspectrogram.shape"]},{"cell_type":"markdown","id":"7ae0a639","metadata":{"id":"7ae0a639"},"source":["Note that the shape of the array (K x T) represents the number of frequency bands (K) and the number of spectral vectors in this representation (here, K=13, T=43). K is a hyperparameter and the recommended values in ASR research are (13, 39, 81, etc). Here, we fix K = 13. On the other hand, T varies from sample to sample depending on the duration of the sample.  "]},{"cell_type":"code","execution_count":null,"id":"f17e8976","metadata":{"id":"f17e8976"},"outputs":[],"source":["# plot and view the spectrogram\n","\n","fig, ax = plt.subplots(figsize=(10, 2), sharex=True)\n","\n","img = librosa.display.specshow(\n","    melspectrogram, \n","    sr=sr, \n","    x_axis='time', \n","    y_axis='mel', \n","    cmap='viridis', \n","    fmax=4000, \n","    hop_length=80\n",")\n","\n","ax.set(title='Log-frequency power spectrogram')\n","\n","ax.label_outer()"]},{"cell_type":"markdown","id":"e6c872b6","metadata":{"id":"e6c872b6"},"source":["As you can see above from the figure, the spectrogram representation can be viewed as a matrix $\\mathbf{X} \\in \\mathbb{R}^{T} \\times \\mathbb{R}^{k}$.  "]},{"cell_type":"markdown","id":"d5392b92","metadata":{"id":"d5392b92"},"source":["## Task I\n","1. One problem with the spectrogram as a speech feature represetation is that different speech samples would have dfferent durations due to inherent speech variability (e.g., speech rate, speaker dialect, etc). That is, the $T$ in the $(T \\times k)$-dimensional representation would be different for each sample. Therefore, for the baseline model, we will implement a method to have a fixed-size representation for all speech samples. Write a function downsample_spectrogram(X, N) that takes as input a spectrogram $\\mathbf{X} \\in \\mathbb{R}^{T \\times k}$ and a parameter N <= 25. The function should (1) make N equally-sized splits of S across the time-axis, (2) apply a pooling technique (e.g., mean pooling) to each split across the frequency axis to obtain an array that represents a downsampled version of the spectrogram $\\mathbf{X}' \\in \\mathbb{R}^{N \\times k}$, and (3) re-arange $\\mathbf{X}'$ as a vector $\\mathbf{v} \\in \\mathbb{R}^{Nk}$.    \n","\n","2. Using the downsample_spectrogram(X, N) function, transform all the speech samples into vectors $\\mathbf{v} \\in \\mathbb{R}^{Nk}$. \n","\n","3. Given the speaker-based train/dev/test spilts in the SDR_metadata.tsv, fit a linear model on the training samples. That is, your model should be build on data from 4 speakers {'nicolas', 'theo' , 'jackson',  'george'}. Hint: you can experiment with a few model alternatives in the SGDClassifier module in scikit-learn. \n","\n","4. Evaluate you model on the dev and test splits. Use accuracy as an evaluation metric. Analyze the model performance using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) of the all possible labels (0-9), Analyze [precision, recall](https://en.wikipedia.org/wiki/Precision_and_recall), [F1-score](https://en.wikipedia.org/wiki/F-score) for each label. Report your observation."]},{"cell_type":"code","execution_count":20,"id":"423a3e64","metadata":{"id":"423a3e64"},"outputs":[],"source":["def ResampleLinear1D(row, N: int): \n","    # validation\n","    if N > 25:\n","        N = 25\n","    if N < 1:\n","        N = 1\n","\n","    # row is a list, need it to be np.array\n","    row = np.array(row, dtype=np.float)\n","    # create new spacing\n","    index_arr = np.linspace(0, row.shape[0]-1, num=N, dtype=np.float)\n","    \n","    index_floor = np.floor(index_arr).astype(np.int) # np.int as they'll return them as np.float types\n","    index_ceil = index_floor + 1 \n","    index_rem = index_arr - index_floor \n","    interp = (row[index_floor] * (1.0 - index_rem)) + \\\n","                (row[index_ceil % row.shape[0]] * index_rem)\n","    return interp\n","\n","def downsample_spectrogram(X, N):\n","    \"\"\"\n","    Given a spectrogram of an arbitrary length/duration (X ∈ K x T), \n","    return a downsampled version of the spectrogram v ∈ K * N\n","    \"\"\"\n","    # use a for loop to downsample the array along the columns\n","    downsampled_arr = []\n","    for row in X:\n","        downsampled_row = ResampleLinear1D(row, N)\n","        downsampled_arr.append(downsampled_row)\n","\n","    v = np.array(downsampled_arr).flatten()\n","     \n","    return v"]},{"cell_type":"markdown","id":"1bbe63ed","metadata":{"id":"1bbe63ed"},"source":["## Baseline Model"]},{"cell_type":"code","execution_count":21,"id":"c53298ad","metadata":{"id":"c53298ad"},"outputs":[],"source":["# prepare data and split\n","def downsaple_all_data(sdr_df, N = 25, partial = False):\n","  train_X = []\n","  dev_X = []\n","  test_X = []\n","\n","  train_Y = []\n","  dev_Y = []\n","  test_Y = []\n","\n","  for index, row in sdr_df.iterrows():\n","      sample_wav_file = row[\"file\"]\n","      x, sr = librosa.load(path + sample_wav_file, sr=SAMPLING_RATE) #, \n","      melspectrogram = extract_melspectrogram(x, sr, num_mels=13)\n","      \n","      downsample = downsample_spectrogram(melspectrogram, N)\n","      label = row[\"label\"]\n","      split = row[\"split\"]\n","      # when we wish to have only a few speakers\n","      if partial:\n","        speaker = row[\"speaker\"]\n","\n","\n","        if split == \"TRAIN\" and speaker in {'nicolas', 'theo' , 'jackson',  'george'}:\n","            train_X.append(downsample)\n","            train_Y.append(label)\n","      else:\n","        if split == \"TRAIN\":\n","            train_X.append(downsample)\n","            train_Y.append(label)\n","      \n","      if split == \"DEV\":\n","          dev_X.append(downsample)\n","          dev_Y.append(label)\n","          \n","      if split == \"TEST\":\n","          test_X.append(downsample)\n","          test_Y.append(label)\n","\n","  train_X = np.array(train_X)\n","  train_Y = np.array(train_Y)\n","\n","  dev_X = np.array(dev_X)\n","  dev_Y = np.array(dev_Y)\n","\n","  test_X = np.array(test_X)\n","  test_Y = np.array(test_Y)\n","    \n","  return train_X, dev_X, test_X, train_Y, dev_Y, test_Y\n","\n"]},{"cell_type":"code","execution_count":22,"id":"uctknAIkUAPD","metadata":{"id":"uctknAIkUAPD"},"outputs":[],"source":["train_X, dev_X, test_X, train_y, dev_y, test_y = downsaple_all_data(sdr_df, N = 20)"]},{"cell_type":"code","execution_count":null,"id":"GjPrTmn5Ve2o","metadata":{"id":"GjPrTmn5Ve2o"},"outputs":[],"source":["X_partial_train, X_partial_dev, X_partial_test, y_partial_train, y_partial_dev, y_partial_test = downsaple_all_data(sdr_df, N = 20, partial=True)"]},{"cell_type":"code","execution_count":null,"id":"u8j_rSVQWUW2","metadata":{"id":"u8j_rSVQWUW2"},"outputs":[],"source":["# train a linear model \n","sgd = SGDClassifier(loss='log_loss', penalty='elasticnet').fit(X_partial_train, y_partial_train)\n","lr = LogisticRegression(solver='saga', multi_class = 'ovr').fit(X_partial_train, y_partial_train)"]},{"cell_type":"code","execution_count":null,"id":"8e0a1059","metadata":{"id":"8e0a1059"},"outputs":[],"source":["# evaluate the model using accuracy metric\n","dev_pred_sgd = sgd.predict(X_partial_dev).astype(\"int\")\n","dev_pred_lr = lr.predict(X_partial_dev).astype(\"int\")\n","\n","score_sgd = accuracy_score(y_partial_dev, dev_pred_sgd)\n","score_lr = accuracy_score(y_partial_dev, dev_pred_lr)\n","print(f'SGDClassifier score {score_sgd}, LogisticRegression score {score_lr}')"]},{"cell_type":"code","execution_count":null,"id":"cde80756","metadata":{"id":"cde80756"},"outputs":[],"source":["# analyze the confusion matrix of the baseline\n","cm_sgd = confusion_matrix(y_partial_dev, dev_pred_sgd)\n","cm_lr = confusion_matrix(y_partial_dev, dev_pred_lr)\n","print('SGD_CM: ', cm_sgd)\n","print('SGD_LR: ', cm_lr)"]},{"cell_type":"code","execution_count":null,"id":"3ea9c6d6","metadata":{"id":"3ea9c6d6"},"outputs":[],"source":["# recall_score(dev_Y, dev_pred_reg, average=\"macro\")\n","print('SGD_Recall: ', recall_score(y_partial_dev, dev_pred_sgd, average=\"macro\"))\n","print('LR_Recall: ', recall_score(y_partial_dev, dev_pred_lr, average=\"macro\"))"]},{"cell_type":"code","execution_count":null,"id":"cYDWT-AEWxei","metadata":{"id":"cYDWT-AEWxei"},"outputs":[],"source":["# f1_score(dev_Y, dev_pred_reg, average=\"macro\")\n","print('SGD_f1: ', f1_score(y_partial_dev, dev_pred_sgd, average=\"macro\"))\n","print('LR_f1: ', f1_score(y_partial_dev, dev_pred_lr, average=\"macro\"))"]},{"cell_type":"markdown","id":"435d62c9","metadata":{"id":"435d62c9"},"source":["## Task II\n","1. Having established a baseline with a linear model trained on a downsampled signal representation of the speech segment, this task aims to learn a classifier based on the full speech segment. To this end, you will implement a neural model that is suitable for sequential data such as recurrent DNN, convolutional DNN with 1-D temporal convolution, or an audio transformer. The model should take the acoustic sample as it is (i.e., the Mel spectrogram could have an arbitrary length) without the need to downsample the segment. You need to implement at least two of the aforementioned models. Do the neural models improve accuracy over the baseline model? Do you observe any signs of overfitting to the training data? How do the hyperparameters affect the model performance? Report and discuss your observations.        \n","\n","2. Evaluate your (best) neural models and compare to the baseline model using the same evalution process as in task I.4. \n","\n","3. Use a dimensionality reduction algorithm such as t-SNE \\[[1](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding),[2](https://pypi.org/project/tsne-torch/),[3](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\\] or [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) to analyze how the different models seperate the different classes (the last non-linear layer in your model). Compare to the downsampled representation you used in the baseline and report your observations.\n","\n","4. Are the differences between the different models statistically significant? To answer this question, you need to implement a statistical significance test based on bootstrapping method. To read more how to estiame p-values based on bootstrapping, we recommend the materials on this paper https://aclanthology.org/D12-1091.pdf. Include the baseline model in your evaluation. "]},{"attachments":{},"cell_type":"markdown","id":"QwV85ZYNXpjU","metadata":{"id":"QwV85ZYNXpjU"},"source":["LSTM Model, inspired by https://towardsdatascience.com/recurrent-neural-nets-for-audio-classification-81cb62327990"]},{"cell_type":"code","execution_count":null,"id":"bTMl42b7Xsot","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"elapsed":404,"status":"error","timestamp":1679506429712,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"bTMl42b7Xsot","outputId":"a89ecf92-351f-4f0e-db90-d4a3ee52557a"},"outputs":[],"source":["# reshape np.array into 3D\n","X_train = train_X.reshape(train_X.shape[0],train_X.shape[1],1)"]},{"cell_type":"code","execution_count":null,"id":"k-qiP9ZdZKeN","metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1679506429713,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"k-qiP9ZdZKeN"},"outputs":[],"source":["timesteps = X_train.shape[1] # how many values exist in a sequence, dimensionality of the input sequence\n","n_outputs = 10\n","units = timesteps # how many LSTM cell we've connected sequentially, at most it's timesteps\n","epochs = 100"]},{"cell_type":"code","execution_count":null,"id":"JdP-9zIIYlvo","metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1679506429713,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"JdP-9zIIYlvo"},"outputs":[],"source":["# data is X_train, num_layers is architecture of hidden layers, number and size, and dropout is values for dropout in hidden layers\n","def get_LSTM(data:np.ndarray, timesteps:int, units:int, num_layers:list(), layer_type:list(), dropout:list(), activation:str):\n","    '''\n","        units: number of units in each LSTM cell, usually set to timesteps\n","        num_layers, layer_type and dropout are with respect to the hidden layer, and need to be of the same length\n","        num_layers: the number of neurons in each hidden layer\n","        layer_type: 1 for LSTM, o.w. for Dense\n","        dropout: self explanatory\n","    '''\n","    if len(num_layers) != len(dropout) and (len(num_layers) or len(dropout)) == 0:\n","        raise Exception('Number of layers and dropout values, has to be the same')\n","    # input shape is timestamp x 1, since we're feeding them sequentially, i.e. 1 is dimensionality of each input representation in the sequence\n","    input_shape=(data.shape[1],1) \n","    model = keras.Sequential()\n","    \n","    # We need to add return_sequences=True for all LSTM layers except the last one.\n","    # Setting this flag to True lets Keras know that LSTM output should contain all historical generated outputs along with time stamps (3D). \n","    # So, next LSTM layer can work further on the data. If this flag is false, then LSTM only returns last output (2D). \n","    # Such output is not good enough for another LSTM layer.\n","\n","    # input layer\n","    if 1 in layer_type:\n","        model.add(LSTM(units,input_shape=input_shape, return_sequences=True))\n","    else:\n","        model.add(LSTM(units,input_shape=input_shape, return_sequences=False))\n","\n","    for i in range(len(num_layers)):\n","        model.add(Dropout(dropout[i])) # applies to the next layer   \n","        if layer_type[i] == 1:\n","            if 1 in layer_type[i+1:]:\n","                model.add(LSTM(units, return_sequences=True))\n","            else:\n","                model.add(LSTM(units))\n","        else:\n","            model.add(Dense(num_layers[i], activation=activation))\n","\n","    # output layer\n","    model.add(Dense(n_outputs, activation='softmax'))\n","    # model.summary()\n","    return model"]},{"attachments":{},"cell_type":"markdown","id":"9fbc634c","metadata":{},"source":["It can be observed that making the network more complex, i.e. increasing the number of hidden layers, increasing the size of the hidden layers, or increasing the number of units, makes it better suited to capture more complex relations between the output and input. This risks the network overfitting the training dataset, and having a poor performance on the testing dataset. We can also observe that having dropout improves the networks reliability and accuracy, by having a sort of regularization put in place. And with LSTM, it has been shown that relu and leaky relu don't suffer as much from the issues of exploding/vanishing gradient, as much as the sigmoid and tanh activation functions."]},{"cell_type":"code","execution_count":null,"id":"tfAAA-P0ZAbp","metadata":{"id":"tfAAA-P0ZAbp"},"outputs":[],"source":["model_lstm = get_LSTM(\n","    data=X_train, \n","    timesteps=timesteps, \n","    units=units, \n","    num_layers=[128, 128, 64, 16, 2], \n","    layer_type=[0, 1, 0, 1, 0], \n","    dropout=[0.25, 0.25, 0.25, 0.25, 0.25], \n","    activation='relu')"]},{"cell_type":"code","execution_count":null,"id":"_1H12bhuZxHx","metadata":{"id":"_1H12bhuZxHx"},"outputs":[],"source":["# options for loss are 'categorical_hinge', 'SparseCategoricalCrossentropy', 'tf.keras.losses.Hinge', 'CategoricalCrossentropy',..\n","model_lstm.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"id":"Jwn5g8vtZNvE","metadata":{"id":"Jwn5g8vtZNvE"},"outputs":[],"source":["# one hot encoding for the outputs\n","encoded_train = to_categorical(train_y)\n","encoded_test = to_categorical(test_y)\n","encoded_dev = to_categorical(dev_y)"]},{"cell_type":"code","execution_count":null,"id":"axQDBWoZZdGW","metadata":{"id":"axQDBWoZZdGW"},"outputs":[],"source":["# will keep the historic results for the acc, loss, val_loss and val_acc \n","history = model_lstm.fit(X_train, encoded_train, epochs=epochs, batch_size=64, validation_data=(dev_X, encoded_dev), shuffle=True)"]},{"cell_type":"code","execution_count":null,"id":"fDqp74DbZb99","metadata":{"id":"fDqp74DbZb99"},"outputs":[],"source":["# show data\n","history_dict_lstm=history.history\n","loss_values=history_dict_lstm['loss']\n","acc_values=history_dict_lstm['acc']\n","val_loss_values = history_dict_lstm['val_loss']\n","val_acc_values=history_dict_lstm['val_acc']\n","epoch=range(1,epochs + 1) # need an x-axis\n","fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n","ax1.plot(epoch,loss_values,'co',label='Training Loss')\n","ax1.plot(epoch,val_loss_values,'m', label='Validation Loss')\n","ax1.set_title('Training and validation loss')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss')\n","ax1.legend()\n","ax2.plot(epoch,acc_values,'co', label='Training accuracy')\n","ax2.plot(epoch,val_acc_values,'m',label='Validation accuracy')\n","ax2.set_title('Training and validation accuracy')\n","ax2.set_xlabel('Epochs')\n","ax2.set_ylabel('Accuracy')\n","ax2.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"eaPaJtDMfZlZ","metadata":{"id":"eaPaJtDMfZlZ"},"outputs":[],"source":["# but these are one-hot encoded\n","dev_pred_lstm_encoded = model_lstm.predict(dev_X).astype(\"int\")"]},{"cell_type":"code","execution_count":null,"id":"fsUEPth5gGp1","metadata":{"id":"fsUEPth5gGp1"},"outputs":[],"source":["# turning back into values\n","dev_pred_lstm=np.argmax(dev_pred_lstm_encoded, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"QsvEIMj7fmLq","metadata":{"id":"QsvEIMj7fmLq"},"outputs":[],"source":["score_lstm = accuracy_score(dev_y, dev_pred_lstm)"]},{"cell_type":"code","execution_count":null,"id":"_m6cgt6uhMpd","metadata":{"id":"_m6cgt6uhMpd"},"outputs":[],"source":["cm_lstm = confusion_matrix(dev_y, dev_pred_lstm)"]},{"attachments":{},"cell_type":"markdown","id":"Vwix-6_uaUW8","metadata":{"id":"Vwix-6_uaUW8"},"source":["CNN Model, inspired by https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610"]},{"cell_type":"code","execution_count":null,"id":"oXbPfNm4aWpi","metadata":{"id":"oXbPfNm4aWpi"},"outputs":[],"source":["n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], 10"]},{"cell_type":"code","execution_count":null,"id":"h7JUSwYIaoIx","metadata":{"id":"h7JUSwYIaoIx"},"outputs":[],"source":["def get_CNN(filters:list(), kernels:list(), activations:list(), dropout:float, pooling:int):\n","    '''\n","        The CNN is made of Conv1D layers, in some order described by the user, usually followed by a dropout layer in order \n","        to avoid overfitting. After it they're followed by a Pooling filter, in order to drop the number of parameters and save\n","        computational costs. At the end there's a Flattening layer and a Fully connected - Dense layer for the output.\n","    '''\n","    if len(filters) != len(kernels) or len(filters) != len(activations):\n","        return None\n","    \n","    model = keras.Sequential()\n","    \n","    for i in range(0, len(filters), 2):\n","        if i == 0:        \n","            model.add(Conv1D(filters=filters[i], kernel_size=kernels[i], activation=activations[i], input_shape=(n_timesteps,n_features)))\n","        else:\n","            model.add(Conv1D(filters=filters[i], kernel_size=kernels[i], activation=activations[i]))\n","        if i+1 == len(filters):\n","            continue\n","        model.add(Conv1D(filters=filters[i+1], kernel_size=kernels[i+1], activation=activations[i+1]))\n","        model.add(Dropout(dropout))\n","        model.add(MaxPooling1D(pool_size=pooling))\n","\n","    \n","    model.add(Flatten())\n","    model.add(Dense(50, activation='relu')) # this was 150, but changing it to 3 layers of 50 improved results\n","    model.add(Dense(4, activation='relu'))\n","    model.add(Dense(2, activation='relu'))\n","    model.add(Dense(n_outputs, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n","\n","    return model"]},{"attachments":{},"cell_type":"markdown","id":"a68f205b","metadata":{},"source":["As was the case with the LSTM, here as well, and NN in general, the more we increase the size of the network the more complexity we can capture. To clarify further, we can increse the complexity of a NN, CNN in this case, in one of two ways (this is an oversimplification and excludes other more fine ways: number of filters, kernel sizes, stride, complexity of activation function, i.e. ceteris paribus - all else being equal): we either increase the depth, or widht of it. \n","\n","Now in accordance with the universal approximation theorem, if we had just one hidden layer, with a non-linear activation function, we can already approximate any (Borel measurable) function, given enough training data. On the extreme, that would mean that just having one, increadibly wide, hidden layer, and huge amount of data, we can train a pretty decent network. The problem with this extreme is that, shallow (wide) networks are really good at memorizing, but bad at generalizing. So if we gave it every possible input it could ever encounter then yes, but since that is hardly the case, this type of network would perform badly on new data, which it hasn't seen. Also as we have seen, the wider the network, the more time it takes to get trained (ceteris paribus).\n","\n","The advantage of adding layers, in depth, is that by increasing the number of layers, we increase the number of intermediate features that the network can learn. So in the case of CNN, we can make the analogy that the first layers might recognize basic things, like edges or colours (again these are mainly used in Computer Vision tasks, not NLP, so this is the running example), but the later layers will be able to recognize more complex shapes and formes, like faces, objects, patterns, etc. This means that the deeper the network the more generalizable they are. That is also one of the reason why, it's been shown that using transfer learning of networks from different domains gives better results (say transfer learning from NN trained on images, to NN that will work on NLP)."]},{"cell_type":"code","execution_count":null,"id":"--lR6l7iax5n","metadata":{"id":"--lR6l7iax5n"},"outputs":[],"source":["# for CNN relu gives better results than tanh, in our case\n","model_cnn = get_CNN([84, 84, 64, 32, 16], [5, 2, 2, 2, 2], ['relu', 'relu', 'relu', 'relu', 'relu'], 0.25, 2)"]},{"cell_type":"code","execution_count":null,"id":"nzmISY4Ea5lS","metadata":{"id":"nzmISY4Ea5lS"},"outputs":[],"source":["verbose, epochs, batch_size = 1, 100, 64"]},{"cell_type":"code","execution_count":null,"id":"BN4PawydbGDi","metadata":{"id":"BN4PawydbGDi"},"outputs":[],"source":["history = model_cnn.fit(train_X, encoded_train, epochs=epochs, batch_size=batch_size, validation_data=(dev_X, encoded_dev), shuffle=True, verbose=verbose)"]},{"cell_type":"code","execution_count":null,"id":"Pnv66Vi8bHsq","metadata":{"id":"Pnv66Vi8bHsq"},"outputs":[],"source":["_, accuracy = model_cnn.evaluate(test_X, encoded_test, batch_size=batch_size, verbose=0)"]},{"cell_type":"code","execution_count":null,"id":"-5I4x32mcLlw","metadata":{"id":"-5I4x32mcLlw"},"outputs":[],"source":["history_dict_cnn=history.history\n","loss_values=history_dict_cnn['loss']\n","acc_values=history_dict_cnn['acc']\n","val_loss_values = history_dict_cnn['val_loss']\n","val_acc_values=history_dict_cnn['val_acc']\n","epoch=range(1,epochs + 1) # need an x-axis\n","fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n","ax1.plot(epoch,loss_values,'co',label='Training Loss')\n","ax1.plot(epoch,val_loss_values,'m', label='Validation Loss')\n","ax1.set_title('Training and validation loss')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss')\n","ax1.legend()\n","ax2.plot(epoch,acc_values,'co', label='Training accuracy')\n","ax2.plot(epoch,val_acc_values,'m',label='Validation accuracy')\n","ax2.set_title('Training and validation accuracy')\n","ax2.set_xlabel('Epochs')\n","ax2.set_ylabel('Accuracy')\n","ax2.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"j_lKe7FXgwVZ","metadata":{"id":"j_lKe7FXgwVZ"},"outputs":[],"source":["dev_pred_cnn_encoded = model_cnn.predict(dev_X).astype(\"int\")"]},{"cell_type":"code","execution_count":null,"id":"jWCbjVuDg4xx","metadata":{"id":"jWCbjVuDg4xx"},"outputs":[],"source":["dev_pred_cnn=np.argmax(dev_pred_cnn_encoded, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"A4FpARXVg7Ng","metadata":{"id":"A4FpARXVg7Ng"},"outputs":[],"source":["score_cnn = accuracy_score(dev_y, dev_pred_cnn)"]},{"cell_type":"code","execution_count":null,"id":"wEhfyGhhhDiE","metadata":{"id":"wEhfyGhhhDiE"},"outputs":[],"source":["cm_cnn = confusion_matrix(dev_y, dev_pred_cnn)"]},{"attachments":{},"cell_type":"markdown","id":"45Si6fDIdBAD","metadata":{"id":"45Si6fDIdBAD"},"source":["Transformers Model, inspired by https://keras.io/examples/timeseries/timeseries_transformer_classification/"]},{"cell_type":"code","execution_count":null,"id":"M29aPrB3dEHp","metadata":{"id":"M29aPrB3dEHp"},"outputs":[],"source":["def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Normalization and Attention\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    return x + res"]},{"cell_type":"code","execution_count":null,"id":"HTObVaTrdWod","metadata":{"id":"HTObVaTrdWod"},"outputs":[],"source":["n_classes = 10\n","def get_Transformer(\n","    input_shape,\n","    head_size,\n","    num_heads,\n","    ff_dim,\n","    num_transformer_blocks,\n","    mlp_units,\n","    dropout=0,\n","    mlp_dropout=0,\n","):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","    # added improves results\n","    x = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same')(x)\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n","    return keras.Model(inputs, outputs)"]},{"cell_type":"code","execution_count":null,"id":"fuDf9lYXdlAI","metadata":{"id":"fuDf9lYXdlAI"},"outputs":[],"source":["input_shape = X_train.shape[1:]\n","head_size = 32\n","num_heads = 8\n","num_transformer_blocks = 8\n","mlp_units = [64]\n","mlp_dropout = 0.2\n","dropout = 0.2\n","epochs = 2\n","lr = 5e-4\n","patience = 10"]},{"cell_type":"code","execution_count":null,"id":"qn8c93Kuds5U","metadata":{"id":"qn8c93Kuds5U"},"outputs":[],"source":["model_transformer = get_Transformer(\n","    input_shape,\n","    head_size=head_size,\n","    num_heads=num_heads,\n","    ff_dim=4,\n","    num_transformer_blocks=num_transformer_blocks,\n","    mlp_units=mlp_units,\n","    mlp_dropout=mlp_dropout,\n","    dropout=dropout,\n",")"]},{"cell_type":"code","execution_count":null,"id":"TlO-8PyCd6ml","metadata":{"id":"TlO-8PyCd6ml"},"outputs":[],"source":["model_transformer.compile(\n","    loss=\"categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=lr),\n","    metrics=[\"acc\"],\n",")\n","\n","# for testing\n","model_transformer.summary()"]},{"cell_type":"code","execution_count":null,"id":"VYsfHDOmeCCq","metadata":{"id":"VYsfHDOmeCCq"},"outputs":[],"source":["callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=patience, restore_best_weights=True)]"]},{"cell_type":"code","execution_count":null,"id":"oAI1o63DeFeD","metadata":{"id":"oAI1o63DeFeD"},"outputs":[],"source":["history = model_transformer.fit(\n","    train_X,\n","    encoded_train,\n","    validation_data=(dev_X, encoded_dev),\n","    epochs=epochs,\n","    batch_size=64,\n","    callbacks=callbacks,\n",")"]},{"cell_type":"code","execution_count":null,"id":"T9Geg-mMeaIy","metadata":{"id":"T9Geg-mMeaIy"},"outputs":[],"source":["evaluation = model_transformer.evaluate(test_X, encoded_test, verbose=0)\n"]},{"cell_type":"code","execution_count":null,"id":"E8eo_aP8eptG","metadata":{"id":"E8eo_aP8eptG"},"outputs":[],"source":["history_dict_transformer=history.history\n","loss_values=history_dict_transformer['loss']\n","acc_values=history_dict_transformer['acc']\n","val_loss_values = history_dict_transformer['val_loss']\n","val_acc_values=history_dict_transformer['val_acc']\n","epoch=range(1,epochs + 1) # need an x-axis\n","fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n","ax1.plot(epoch,loss_values,'co',label='Training Loss')\n","ax1.plot(epoch,val_loss_values,'m', label='Validation Loss')\n","ax1.set_title('Training and validation loss')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss')\n","ax1.legend()\n","ax2.plot(epoch,acc_values,'co', label='Training accuracy')\n","ax2.plot(epoch,val_acc_values,'m',label='Validation accuracy')\n","ax2.set_title('Training and validation accuracy')\n","ax2.set_xlabel('Epochs')\n","ax2.set_ylabel('Accuracy')\n","ax2.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"LLEr03AWhYPP","metadata":{"id":"LLEr03AWhYPP"},"outputs":[],"source":["dev_pred_transformer_encoded = model_transformer.predict(dev_X).astype(\"int\")"]},{"cell_type":"code","execution_count":null,"id":"vPWhTB60hYyt","metadata":{"id":"vPWhTB60hYyt"},"outputs":[],"source":["dev_pred_transformer=np.argmax(dev_pred_transformer_encoded, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"9vlAcm8uhY88","metadata":{"id":"9vlAcm8uhY88"},"outputs":[],"source":["score_transformer = accuracy_score(dev_y, dev_pred_transformer)"]},{"cell_type":"code","execution_count":null,"id":"QTfC3iIEhZMa","metadata":{"id":"QTfC3iIEhZMa"},"outputs":[],"source":["cm_transformer = confusion_matrix(dev_y, dev_pred_transformer)"]},{"cell_type":"markdown","id":"8_ixxTtn0wgx","metadata":{"id":"8_ixxTtn0wgx"},"source":["Comparison"]},{"attachments":{},"cell_type":"markdown","id":"u1UEpZyxIOFT","metadata":{"id":"u1UEpZyxIOFT"},"source":["https://github.com/Kaleidophon/deep-significance#scenario-4---comparing-more-than-two-models\n","\n","\n","tl;dr: Use aso() to compare scores for two models. If the returned eps_min < 0.5, A is better than B. The lower eps_min, the more confident the result (we recommend to check eps_min < 0.2 and record eps_min alongside experimental results)."]},{"cell_type":"code","execution_count":null,"id":"IDBH7WKU0wNn","metadata":{"id":"IDBH7WKU0wNn"},"outputs":[],"source":["lstm_np = np.array(history_dict_lstm['acc'])\n","cnn_np = np.array(history_dict_cnn['acc'])\n","transformer_np = np.array(history_dict_transformer['acc'])\n","\n","\n","my_models_scores = np.stack([lstm_np, cnn_np, transformer_np], axis=0)\n","eps_min = multi_aso(my_models_scores, confidence_level=0.95, return_df=True)"]},{"cell_type":"markdown","id":"8f95e312","metadata":{"id":"8f95e312"},"source":["## Task III (Open Ended)\n","1. Consider the case where we have speech data from a single speaker (e.g., george). Train your models on this subset of the data. What do you observe? How does this affect the model performance? \n","\n","2. Even though a model is trained on a single speaker, we would like the model to generalizes to any speaker. To this end, one can use data augmentation techniques to artificially create more samples for each class. Some of these augmentations can be applied on the spectrogram (e.g., SpecAugment https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html), and other can be applied on the raw waveform before creating the spectrogram such as pitch manipulation (https://github.com/facebookresearch/WavAugment). Explore the effect of one type of augmentation from each type. Report your observation and anaylze the confusion matrices.\n","\n","3. Data augmentation techniques create different \"views\" of each training sample in a stochastic or determinstic approach. One can leaverage speech data augmentation to create views for training a neural network in a contrastive learning setting with margin-based objective function (for more info, read http://proceedings.mlr.press/v130/al-tahan21a/al-tahan21a.pdf). Implement at least one model using a contrastive loss based on different views of the training samples. Does this model improve over the model without contrastive learning? Report and discuss your observations. \n","\n","For more information on the contrastive learning framework, you can refer to this paper\n","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226466"]},{"attachments":{},"cell_type":"markdown","id":"7fcc91b7","metadata":{},"source":["Code for augmentation inspired by https://jonathanbgn.com/2021/08/30/audio-augmentation.html. It is used mainly to demonstrate the effects of data augmentation, i.e. it improves prediction accuracy."]},{"cell_type":"code","execution_count":null,"id":"6aab3876","metadata":{"id":"6aab3876"},"outputs":[],"source":["class RandomClip:\n","    '''\n","        Create a new audio sample, from a random clip from the original.\n","    '''\n","    def __init__(self, sample_rate, clip_length):\n","        self.clip_length = clip_length\n","        self.vad = torchaudio.transforms.Vad(\n","            sample_rate=sample_rate, trigger_level=7.0)\n","\n","    def __call__(self, audio_data):\n","        audio_length = audio_data.shape[0]\n","        if audio_length > self.clip_length:\n","            offset = random.randint(0, audio_length-self.clip_length)\n","            audio_data = audio_data[offset:(offset+self.clip_length)]\n","\n","        # remove silences at the beggining/end\n","        return self.vad(audio_data) \n","\n","# clip_transform = RandomClip(SAMPLING_RATE, 1000*len(x)) # 8 seconds clip\n","# transformed_audio = clip_transform(sdr_df[sdr_df['file'] == sample_wav_file]['file'])"]},{"cell_type":"code","execution_count":null,"id":"odEvF1K5nBOG","metadata":{"id":"odEvF1K5nBOG"},"outputs":[],"source":["class RandomSpeedChange:\n","    ''' \n","        Change the speed of the audio.\n","    '''\n","    def __init__(self, sample_rate):\n","        self.sample_rate = sample_rate\n","\n","    def __call__(self, audio_data):\n","        speed_factor = random.choice([0.9, 1.0, 1.1])\n","        if speed_factor == 1.0: # no change\n","            return audio_data\n","\n","        # change speed and resample to original rate:\n","        sox_effects = [\n","            [\"speed\", str(speed_factor)],\n","            [\"rate\", str(self.sample_rate)],\n","        ]\n","        transformed_audio, _ = torchaudio.sox_effects.apply_effects_tensor(\n","            audio_data, self.sample_rate, sox_effects)\n","        return transformed_audio"]},{"cell_type":"code","execution_count":null,"id":"j-kHLcTR1jYd","metadata":{"id":"j-kHLcTR1jYd"},"outputs":[],"source":["class ComposeTransform:\n","    '''\n","        Add the transformations\n","    '''\n","    def __init__(self, transforms):\n","        self.transforms = transforms\n","\n","    def __call__(self, audio_data):\n","        for t in self.transforms:\n","            audio_data = t(audio_data)\n","        return audio_data\n","\n","\n","compose_transform = ComposeTransform([\n","    RandomClip(sample_rate=SAMPLING_RATE, clip_length=3428),\n","    RandomSpeedChange(SAMPLING_RATE)])"]},{"cell_type":"code","execution_count":null,"id":"Uw9rLIvI1o3T","metadata":{"id":"Uw9rLIvI1o3T"},"outputs":[],"source":["def add_white_noise(audio, var=1):\n","  '''\n","    Will add white noise to it.\n","  '''\n","  white_noise = torch.zeros(audio.shape)\n","  white_noise = white_noise + (var**0.5)*torch.randn(audio.shape)\n","  return audio + white_noise"]},{"cell_type":"code","execution_count":null,"id":"wdryHxlw1vAZ","metadata":{"id":"wdryHxlw1vAZ"},"outputs":[],"source":["def augment_data(sdr_df, N=24):\n","  ''' \n","    Will create new augmented data, but it will return it separately from the original data. \n","    Needs to be concatinated further. Also X_augmented is 2D, (#samples, length of samples),\n","    while X[_train] is 3D (#samples, length of samples, 1)\n","  '''\n","  X_train_augmented = []\n","  X_dev_augmented = []\n","  X_test_augmented = []\n","\n","  y_train_augmented = []\n","  y_dev_augmented = []\n","  y_test_augmented = []\n","\n","  for index, row in sdr_df.iterrows():\n","      sample_wav_file = row[\"file\"]\n","      x, sr = librosa.load(path + sample_wav_file, sr=SAMPLING_RATE)\n","      clip_length = len(x)\n","\n","      audio_file, sample_rate = torchaudio.load(path + sample_wav_file)      \n","      transformed_audio = compose_transform(audio_file)\n","      add_white_noise(transformed_audio)\n","\n","      # transformed_audio = np.array(transformed_audio) transformed_audio is Tensor, so -> numpy (but has shape (1, ...), and we need (..., ))\n","      melspectrogram = extract_melspectrogram(transformed_audio.numpy().reshape(transformed_audio.numpy().shape[1]), sr, num_mels=13)\n","      \n","      downsample = downsample_spectrogram(melspectrogram, N)\n","      label = row[\"label\"]\n","      split = row[\"split\"]\n","      \n","      if split == \"TRAIN\":\n","          X_train_augmented.append(downsample)\n","          y_train_augmented.append(label)\n","      \n","      if split == \"DEV\":\n","          X_dev_augmented.append(downsample)\n","          y_dev_augmented.append(label)\n","          \n","      if split == \"TEST\":\n","          X_test_augmented.append(downsample)\n","          y_test_augmented.append(label)\n","\n","  X_train_augmented = np.array(X_train_augmented)\n","  y_train_augmented = np.array(y_train_augmented)\n","  X_dev_augmented = np.array(X_dev_augmented)\n","  y_dev_augmented = np.array(y_dev_augmented)\n","  X_test_augmented = np.array(X_test_augmented)\n","  y_test_augmented = np.array(y_test_augmented)\n","\n","  return X_train_augmented, X_dev_augmented, X_test_augmented, y_train_augmented, y_dev_augmented, y_test_augmented"]},{"cell_type":"code","execution_count":null,"id":"EHczBmS73cY1","metadata":{"id":"EHczBmS73cY1"},"outputs":[],"source":["X_train_augmented, X_dev_augmented, X_test_augmented, y_train_augmented, y_dev_augmented, y_test_augmented = augment_data(sdr_df, N=20)"]},{"cell_type":"code","execution_count":null,"id":"9rajf7zN3vVT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1679431397284,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"9rajf7zN3vVT","outputId":"12898aaa-4c25-4c04-ed2d-465be10687cd"},"outputs":[],"source":["X_train_augmented.shape, X_train.shape"]},{"cell_type":"code","execution_count":null,"id":"zOJErtYx5cp5","metadata":{"id":"zOJErtYx5cp5"},"outputs":[],"source":["X_train_augmented = X_train_augmented.reshape(X_train_augmented.shape[0], X_train_augmented.shape[1], 1)\n","X_test_augmented = X_test_augmented.reshape(X_test_augmented.shape[0], X_test_augmented.shape[1], 1)\n","X_dev_augmented = X_dev_augmented.reshape(X_dev_augmented.shape[0], X_dev_augmented.shape[1], 1)"]},{"cell_type":"code","execution_count":null,"id":"68616def","metadata":{},"outputs":[],"source":["X_train_augmented.shape, train_X.shape"]},{"cell_type":"code","execution_count":null,"id":"H_M22YUN462p","metadata":{"id":"H_M22YUN462p"},"outputs":[],"source":["X_train_concatenated = np.concatenate([X_train_augmented, train_X.reshape(X_train_augmented.shape[0], train_X.shape[1], 1)], axis=0)\n","X_test_concatenated = np.concatenate([X_test_augmented, test_X.reshape(test_X.shape[0], test_X.shape[1], 1)], axis=0)\n","X_dev_concatenated = np.concatenate([X_dev_augmented, dev_X.reshape(dev_X.shape[0], dev_X.shape[1], 1)], axis=0)\n","\n","y_train_concatenated = np.concatenate([y_train_augmented, train_y], axis=0)\n","y_test_concatenated = np.concatenate([y_test_augmented, test_y], axis=0)\n","y_dev_concatenated = np.concatenate([y_dev_augmented, dev_y], axis=0)"]},{"cell_type":"code","execution_count":null,"id":"gzLZDFiq_nEd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1679432822713,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"gzLZDFiq_nEd","outputId":"f8c81351-7d94-4266-d540-05df83a046e3"},"outputs":[],"source":["encoded_train_augmented = to_categorical(y_train_concatenated)\n","encoded_test_augmented = to_categorical(y_test_concatenated)"]},{"cell_type":"code","execution_count":null,"id":"af400b80","metadata":{},"outputs":[],"source":["history_augmented = model_cnn.fit(X_train_concatenated, encoded_train_augmented, epochs=epochs, batch_size=batch_size, validation_data=(dev_X, encoded_dev), shuffle=True, verbose=verbose)"]},{"cell_type":"code","execution_count":null,"id":"8800fa60","metadata":{},"outputs":[],"source":["_, accuracy_augmented = model_cnn.evaluate(X_test_concatenated, encoded_test_augmented, batch_size=batch_size, verbose=0)"]},{"cell_type":"code","execution_count":null,"id":"bae886a8","metadata":{},"outputs":[],"source":["dev_pred_cnn_encoded_augmented = model_cnn.predict(X_dev_concatenated).astype(\"int\")"]},{"cell_type":"code","execution_count":null,"id":"26337a65","metadata":{},"outputs":[],"source":["dev_pred_cnn_augmented=np.argmax(dev_pred_cnn_encoded_augmented, axis=1)"]},{"cell_type":"code","execution_count":null,"id":"3e886cbf","metadata":{},"outputs":[],"source":["score_cnn_augmented = accuracy_score(y_dev_concatenated, dev_pred_cnn_augmented)"]},{"cell_type":"code","execution_count":null,"id":"df8d96b2","metadata":{},"outputs":[],"source":["cm_cnn_augmented = confusion_matrix(y_dev_augmented, dev_pred_cnn)"]},{"cell_type":"code","execution_count":null,"id":"d9ab97d6","metadata":{},"outputs":[],"source":["score_cnn, score_cnn_augmented, cm_cnn, cm_cnn_augmented"]},{"cell_type":"code","execution_count":null,"id":"58c3d1ac","metadata":{},"outputs":[],"source":["type(history_augmented.history), type(history_dict_cnn)"]},{"cell_type":"code","execution_count":null,"id":"fbe7473f","metadata":{},"outputs":[],"source":["augmented_np = np.array(history_augmented.history['acc'])\n","cnn_np = np.array(history_dict_cnn['acc'])\n","\n","\n","my_models_scores = np.stack([augmented_np, cnn_np], axis=0)\n","eps_min = multi_aso(my_models_scores, confidence_level=0.95, return_df=True)"]},{"attachments":{},"cell_type":"markdown","id":"fadbe1c4","metadata":{},"source":["Delete after this, trials."]},{"cell_type":"markdown","id":"ZLYgTE2tIExK","metadata":{"id":"ZLYgTE2tIExK"},"source":["https://keras.io/examples/vision/supervised-contrastive-learning/"]},{"cell_type":"code","execution_count":null,"id":"RyR27-OF6fxT","metadata":{"id":"RyR27-OF6fxT"},"outputs":[],"source":["num_classes = 10\n","input_shape = X_train_concatenated.shape"]},{"cell_type":"code","execution_count":null,"id":"aZ4bjhFx-xqm","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":26320,"status":"error","timestamp":1679434342640,"user":{"displayName":"Georgi Vitanov","userId":"06039515042590683903"},"user_tz":-60},"id":"aZ4bjhFx-xqm","outputId":"6ef37183-6030-48cf-9d5d-9fea4605326d"},"outputs":[],"source":["def create_encoder(X_train_concatenated):\n","\n","    augmented_data = X_train_concatenated.reshape(1, 4000, 260, 1)\n","    resnet = keras.applications.ResNet101V2(\n","        include_top=True, weights=None, input_shape=input_shape, pooling=\"avg\"\n","    )\n","\n","    inputs = keras.Input(shape=input_shape)\n","    outputs = resnet(augmented_data)\n","    # print(type(outputs), outputs.shape)\n","    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n","    # return model\n","\n","\n","encoder = create_encoder(X_train_concatenated)\n","# encoder.summary()"]},{"cell_type":"code","execution_count":null,"id":"h4WhN2ne7RNt","metadata":{"id":"h4WhN2ne7RNt"},"outputs":[],"source":["def make_pairs(audios, labels):\n","\t# initialize two empty lists to hold the (audios, image) pairs and\n","\t# labels to indicate if a pair is positive or negative\n","\tpairAudios = []\n","\tpairLabels = []\n","\t# calculate the total number of classes present in the dataset\n","\t# and then build a list of indexes for each class label that\n","\t# provides the indexes for all examples with a given label\n","\tnumClasses = len(np.unique(labels))\n","\tidx = [np.where(labels == i)[0] for i in range(0, numClasses)]\n","\t# loop over all audios\n","\tfor idxA in range(len(audios)):\n","\t\t# grab the current audio and label belonging to the current\n","\t\t# iteration\n","\t\tcurrentAudio = audios[idxA]\n","\t\tlabel = labels[idxA]\n","\t\t# randomly pick an audio that belongs to the *same* class\n","\t\t# label\n","\t\tidxB = np.random.choice(idx[label])\n","\t\tposAudio = audios[idxB]\n","\t\t# prepare a positive pair and update the audios and labels\n","\t\t# lists, respectively\n","\t\tpairAudios.append([currentAudio, posAudio])\n","\t\tpairLabels.append([1])\n","\t\t# grab the indices for each of the class labels *not* equal to\n","\t\t# the current label and randomly pick an audio corresponding\n","\t\t# to a label *not* equal to the current label\n","\t\tnegIdx = np.where(labels != label)[0]\n","\t\tnegAudio = audios[np.random.choice(negIdx)]\n","\t\t# prepare a negative pair of audios and update our lists\n","\t\tpairAudios.append([currentAudio, negAudio])\n","\t\tpairLabels.append([0])\n","\t# return a 2-tuple of our audio pairs and labels\n","\treturn (np.array(pairAudios), np.array(pairLabels))"]},{"cell_type":"code","execution_count":null,"id":"zJuSRlrI7yr1","metadata":{"id":"zJuSRlrI7yr1"},"outputs":[],"source":["def euclidean_distance(vectors):\n","\t# unpack the vectors into separate lists\n","\t(featsA, featsB) = vectors\n","\t# compute the sum of squared distances between the vectors\n","\tsumSquared = K.sum(K.square(featsA - featsB), axis=1,\n","\t\tkeepdims=True)\n","\t# return the euclidean distance between the vectors\n","\treturn K.sqrt(K.maximum(sumSquared, K.epsilon()))\n"]},{"cell_type":"code","execution_count":null,"id":"G2YNGFdh8C6K","metadata":{"id":"G2YNGFdh8C6K"},"outputs":[],"source":["def contrastive_loss(y, preds, margin=1):\n","\t# explicitly cast the true class label data type to the predicted\n","\t# class label data type (otherwise we run the risk of having two\n","\t# separate data types, causing TensorFlow to error out)\n","\ty = tf.cast(y, preds.dtype)\n","\t# calculate the contrastive loss between the true labels and\n","\t# the predicted labels\n","\tsquaredPreds = K.square(preds)\n","\tsquaredMargin = K.square(K.maximum(margin - preds, 0))\n","\tloss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n","\t# return the computed contrastive loss to the calling function\n","\treturn loss"]},{"cell_type":"code","execution_count":null,"id":"AjpZXct68F7K","metadata":{"id":"AjpZXct68F7K"},"outputs":[],"source":["# Constructing our audio pairs\n","(pairTrain, labelTrain) = make_pairs(X_train_augmented, y_train_augmented)\n","(pairTest, labelTest) = make_pairs(X_test_augmented, y_test_augmented)"]},{"cell_type":"code","execution_count":null,"id":"d5ePmLPs9mZn","metadata":{"id":"d5ePmLPs9mZn"},"outputs":[],"source":["imgA = Input(input_shape[1], input_shape[2])\n","imgB = Input(input_shape[1], input_shape[2]\n","featureExtractor = build_siamese_model(config.IMG_SHAPE)\n","featsA = featureExtractor(imgA)\n","featsB = featureExtractor(imgB)\n","# finally, construct the siamese network\n","distance = Lambda(utils.euclidean_distance)([featsA, featsB])\n","model = Model(inputs=[imgA, imgB], outputs=distance)"]},{"cell_type":"code","execution_count":null,"id":"054bf5f5","metadata":{},"outputs":[],"source":["!pip freeze > requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python --version"]}],"metadata":{"colab":{"collapsed_sections":["61601679-f2f0-437a-8233-54c5ab0cac17","6a0a0884","6ab5f7e7","d2a0c08e","d5392b92","1bbe63ed","435d62c9"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
